{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "from IPython.display import display\n",
    "from bokeh.io import show, output_notebook\n",
    "import glob\n",
    "import re\n",
    "import os.path\n",
    "import math\n",
    "import functools\n",
    "import pickle\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.models import HoverTool, ranges\n",
    "pd.options.mode.chained_assignment = None\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def log(msg): print(dt.now(), msg)\n",
    "\n",
    "class parentree:\n",
    "    def parensplit(self, s, sep = \",\"):\n",
    "        ret = []\n",
    "        cur = \"\"\n",
    "        lvl = 0\n",
    "        for char in s:\n",
    "            if char == \"(\": lvl += 1\n",
    "            elif char == \")\": lvl -=1\n",
    "            if char == sep and lvl == 0:\n",
    "                ret.append(cur)\n",
    "                cur = \"\"\n",
    "            else: cur += char\n",
    "        ret.append(cur)\n",
    "        return ret\n",
    "    def __str__(self):\n",
    "        return self.n + ((\"(\" + \", \".join([ str(item) for item in self.items ]) + \")\") if len(self.items) > 0 else \"\")\n",
    "    def debugstr(self, indent=\"\"):\n",
    "        ret = indent + self.n + \"\\n\"\n",
    "        for item in self.items: ret += item.debugstr(indent + \"  \")\n",
    "        return ret\n",
    "    def __init__(self, s):\n",
    "        match = re.match(\"\\\\s*([^()]*)(\\\\((.*)\\\\))?\\\\s*\", s)\n",
    "        self.n = match.group(1)\n",
    "        self.items = [ parentree(item) for item in self.parensplit(match.group(3)) ] if match.group(3) else []\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "class reader:\n",
    "    eventcol = \"event\"\n",
    "\n",
    "    def tscmp(self, a, b): # Compare two timestamps, treating None as infinitely in the future\n",
    "        if a is None and b is None: return 0\n",
    "        if b is None: return -1\n",
    "        if a is None: return 1\n",
    "        if a == b: return 0\n",
    "        return -1 if a < b else 1\n",
    "    def svccmp(self, tida, tidb, tsa, tsb): # Compare timestamps, but always keep timestamps in a given trace together\n",
    "        if tida < tidb: return -1\n",
    "        if tida > tidb: return 1\n",
    "        return self.tscmp(tsa, tsb)\n",
    "    def rpctree_service(self, event, idx): # Extract the host and port from an RPC: 0 for sender, 1 for receiver\n",
    "        assert(event.n == \"RPC\")\n",
    "        assert(idx in (0, 1))\n",
    "        return tuple(event[idx].n.split(\"/\")[-1].split(\":\"))\n",
    "    def event_initiator(self, uuid, event): # Return the host initiating any event\n",
    "        if event.n == \"RPC\": return self.rpctree_service(event, 0)\n",
    "        else: return self.uuidmap[uuid]\n",
    "    def service_name(self, host, name): # Defines the formatting of a service name\n",
    "        return host[0] + \" \" + host[1] + \" \" + name\n",
    "    def tree_service_name(self, subtree): # Get the name of one of the services involved in an RPC\n",
    "        host = tuple(subtree.n.split(\"/\")[-1].split(\":\"))\n",
    "        return self.service_name(host, self.servicemap[host] if host in self.servicemap else \"\")\n",
    "    def getstarts(self): # Generate start times for each UUID (difficult to do services)\n",
    "        ret = { uuid: None for uuid in self.uuidmap.keys() }\n",
    "        for i in range(len(self.xevents)):\n",
    "            row = self.xevents.iloc[i]\n",
    "            if ret[row[\"id\"]] is None: ret[row[\"id\"]] = row[\"time\"]\n",
    "        return ret\n",
    "    def service_start(self, service): # Check the time of the first event corresponding to a service\n",
    "        targetid = self.uuidrevmap[service]\n",
    "        for i in range(len(self.xevents)):\n",
    "            row = self.xevents.iloc[i]\n",
    "            if row[\"id\"] == targetid: return row[\"time\"]\n",
    "        #start = self.df[self.df.apply(lambda x: self.event_initiator(x[\"id\"], x[self.eventcol]) == service, 1)].sort_values(\"time\")[\"time\"].head(1).tolist()\n",
    "        #if len(start) > 0: return start[0]\n",
    "        return None\n",
    "    def filter_apply_events(self, event, filters): # Sequentially apply all filters to an event\n",
    "        for f in filters:\n",
    "            if not f.fevent(event): return False\n",
    "        return True\n",
    "    def filter_service_basic(self, host, filters): # Sequentially apply all filters to a service\n",
    "        #if host[1] == \"\": return True # Retain unresolvable UUIDs\n",
    "        if host not in self.servicemap: return False\n",
    "        for f in filters:\n",
    "            if not f.fservice(host[0], host[1], self.servicemap[host]): return False\n",
    "        return True\n",
    "    def filter_apply_services(self, uuid, event, filters): # Apply all service filters to an event based on whether it is an RPC\n",
    "        if event.n == \"RPC\":\n",
    "            return self.filter_service_basic(self.rpctree_service(event, 0), filters) and self.filter_service_basic(self.rpctree_service(event, 1), filters)\n",
    "        else:\n",
    "            if uuid not in self.uuidmap: return False\n",
    "            return self.filter_service_basic(self.uuidmap[uuid], filters)\n",
    "    def filter_apply_mutations(self, tree, filters): # Apply mutations from all filters to an event\n",
    "        for f in filters: tree = f.mutate(tree)\n",
    "        return tree\n",
    "    def filter_one(self, df, filters):\n",
    "        if len(df) == 0: return df\n",
    "        df = df[df.apply(lambda x: self.filter_apply_services(x[\"id\"], x[self.eventcol], filters), 1)] # Filter the services\n",
    "        df = df[df[self.eventcol].apply(lambda x: self.filter_apply_events(x, filters))] # Filter the events\n",
    "        df[self.eventcol] = df[self.eventcol].map(lambda x: self.filter_apply_mutations(x, filters)) # Apply mutations to the event\n",
    "        df[\"name\"] = df[\"id\"].map(self.uuid2name)\n",
    "        return df\n",
    "    def uuid2name(self, uuid): # Get the service name corresponding to a UUID\n",
    "        service = self.uuidmap[uuid]\n",
    "        return self.service_name(service, self.servicemap[service])\n",
    "    def host2name(self, host): # Get the service name corresponding to a SparkHost parentree\n",
    "        service = tuple(host.n.split(\"/\")[-1].split(\":\"))\n",
    "        return self. service_name(service, self.servicemap[service])\n",
    "    def makemaps(self):\n",
    "        alluuids = self.df[\"id\"].unique().tolist() # Get a list of all UUIDs\n",
    "        self.uuidrevmap = { self.rpctree_service(row[self.eventcol], 1) : row[\"id\"] for row in self.df[self.df[self.eventcol].apply(lambda x: x.n == \"RPC\")][[\"id\", self.eventcol]].to_dict(\"records\") } # Map from IP and port to trace UUID\n",
    "        missing_uuids = [ uuid for uuid in alluuids if uuid not in self.uuidrevmap.values() ] # UUIDs that are not resolvable\n",
    "        self.uuidrevmap.update({ (uuid, \"\"): uuid for uuid in missing_uuids }) # JVMs that have never received a message are unresolvable.  Thus, map their UUIDs to themselves so they can still be in the plot\n",
    "        self.uuidmap = { val : key for key, val in self.uuidrevmap.items() } # Map indeterminately from trace UUID to IP and port\n",
    "        self.servicemap = { tuple(event[1].n.split(\"/\")[-1].split(\":\")): event[0].n for event in self.df[self.df[self.eventcol].apply(lambda x: x.n == \"Service\")][self.eventcol].drop_duplicates().tolist() } #{ self.rpctree_service(host, 1): host[1][0].n for host in self.df[self.df[self.eventcol].apply(lambda x: x.n == \"RPC\")][self.eventcol].drop_duplicates().tolist() } # Map from IP and port to service name\n",
    "        self.servicemap.update({ (uuid, \"\"): \"\" for uuid in missing_uuids }) # Map unresolved UUIDs back to themselves to handle the case in the line after next\n",
    "        self.df[\"name\"] = self.df[\"id\"].map(self.uuid2name)\n",
    "    def __init__(self, sources):\n",
    "        subtraces = []\n",
    "        self.tracemap = {}\n",
    "        for source in sources: # Load CSVs\n",
    "            curtrace = pd.concat([ pd.read_csv(open(file), sep=\"\\t\") for file in glob.glob(source + \"/*.tsv\") if os.path.getsize(file) > 0 ])\n",
    "            curtrace[\"traceid\"] = len(subtraces)\n",
    "            starttime = curtrace[\"time\"].min() # Find the start time, so that all plots are normalized to start from time 0\n",
    "            curtrace[\"time\"] = pd.to_datetime(curtrace[\"time\"] - starttime, unit=\"ms\") # Add datetime column\n",
    "            for uuid in curtrace[\"id\"].unique().tolist():\n",
    "                self.tracemap[uuid] = len(subtraces)\n",
    "            subtraces.append(curtrace)\n",
    "        self.df = pd.concat(subtraces)\n",
    "        self.df[self.eventcol] = self.df[\"type\"].map(lambda x: parentree(x)) # Add column of parsed case class trees\n",
    "        self.makemaps()\n",
    "        \n",
    "        self.xrpcs = self.df[self.df[self.eventcol].apply(lambda x: x.n == \"RPC\")]\n",
    "        self.xrpcs[\"src\"] = self.xrpcs[self.eventcol].map(lambda x: self.tree_service_name(x[0]))\n",
    "        self.xrpcs[\"dst\"] = self.xrpcs[self.eventcol].map(lambda x: self.tree_service_name(x[1]))\n",
    "        self.xevents = self.df[self.df[self.eventcol].apply(lambda x: x.n not in [\"RPC\", \"SpanStart\", \"SpanEnd\"])]\n",
    "        self.xprocesses = self.df[self.df[self.eventcol].apply(lambda x: x.n in [\"SpanStart\", \"SpanEnd\"])]\n",
    "        self.starttimes = self.getstarts()\n",
    "        servicetmp = sorted([ ((service, name), self.tracemap[self.uuidrevmap[service]], self.starttimes[self.uuidrevmap[service]]) for (service, name) in self.servicemap.items() ], key=functools.cmp_to_key(lambda a, b: self.svccmp(a[1], b[1], a[2], b[2])))\n",
    "        self.xservices = [ svc[0] for svc in servicetmp ]\n",
    "    def filter(self, filters): # Apply filters\n",
    "        self.servicemap = { key: val for key, val in self.servicemap.items() if self.filter_service_basic(key, filters) } # Refresh the service map\n",
    "        self.uuidmap = { val: key for key, val in self.uuidrevmap.items() if self.filter_service_basic(key, filters) } # Refresh UUID map so UUIDs map to non-excluded services when possible\n",
    "        self.df = self.filter_one(self.df, filters)\n",
    "        self.xrpcs = self.filter_one(self.xrpcs, filters)\n",
    "        self.xevents = self.filter_one(self.xevents, filters)\n",
    "        self.xservices = [ svc for svc in self.xservices if svc[0] in self.servicemap.keys() ] # FIXME Update xservices based on new start times\n",
    "        self.xprocesses = self.filter_one(self.xprocesses, filters)\n",
    "    def resolved(self): # Get a table of the resolved services, hosts, and UUIDs\n",
    "        return pd.DataFrame([ (self.tracemap[uuid], uuid, hostport[0], hostport[1], self.servicemap[hostport]) for (uuid, hostport) in self.uuidmap.items() ], columns=[\"T\", \"UUID\", \"Host\", \"Port\", \"Service\"])\n",
    "    def rpcs(self):\n",
    "        ret = self.xrpcs\n",
    "        ret[\"type\"] = ret[self.eventcol].map(str)\n",
    "        del ret[self.eventcol]\n",
    "        return ret\n",
    "    def events(self):\n",
    "        ret = self.xevents\n",
    "        ret[\"type\"] = ret[self.eventcol].map(str)\n",
    "        del ret[self.eventcol]\n",
    "        return ret\n",
    "    def processes(self): # Pull out processes and return as a dataframe\n",
    "        ret = self.xprocesses\n",
    "        if len(ret) == 0: return pd.DataFrame({\"name\": [], \"start\": [], \"end\": []})\n",
    "        ret[\"pid\"] = ret[self.eventcol].map(lambda x: x[0].n)\n",
    "        ret[\"order\"] = ret[self.eventcol].map(lambda x: 0 if x.n == \"SpanStart\" else 1)\n",
    "        ret = ret.pivot(\"pid\", \"order\")\n",
    "        ret = pd.DataFrame({\"id\": ret[\"id\", 0], \"start\": ret[\"time\", 0], \"end\": ret[\"time\", 1], \"type\": ret[\"type\", 0], \"event\": ret[\"event\", 0]})\n",
    "        ret[\"name\"] = ret[\"id\"].map(self.uuid2name)\n",
    "        ret[\"type\"] = ret[self.eventcol].map(lambda x: str(x[1]))\n",
    "        del(ret[self.eventcol])\n",
    "        return ret\n",
    "    def services(self): # Get a list of the services involved in the trace\n",
    "        return [ self.service_name(svc[0], svc[1]) for svc in self.xservices ]\n",
    "    def timerange(self): # Get the start and end time of the trace.\n",
    "        return (self.df[\"time\"].min(), self.df[\"time\"].max())\n",
    "    def debug_display(self): # Dump internal state\n",
    "        display(self.resolved())\n",
    "        display(self.tracemap) # UUID -> trace ID\n",
    "        display(self.uuidrevmap) # (IP, port) -> UUID\n",
    "        display(self.uuidmap) # UUID -> (IP, port)\n",
    "        display(self.servicemap) # (IP, port) -> service name\n",
    "        display(self.starttimes) # UUID -> start time\n",
    "        display(self.df)\n",
    "\n",
    "class displayfilter:\n",
    "    def fevent(event): return True\n",
    "    def fservice(host, port, name): return True\n",
    "    def mutate(tree): return tree\n",
    "\n",
    "def seqplot(trace):\n",
    "    rpcs = trace.rpcs()\n",
    "    events = trace.events()\n",
    "    processes = trace.processes()\n",
    "    services = trace.services()\n",
    "    timerange = trace.timerange()\n",
    "\n",
    "    hover = HoverTool()\n",
    "    hover.tooltips = \"<div style='max-width: 400px; word-wrap: wrap-all'>@type</div>\"\n",
    "    p = figure(y_axis_type=\"datetime\", x_range=services, tools=[\"ypan\", \"ywheel_zoom\", hover, \"reset\"], active_scroll=\"ywheel_zoom\")\n",
    "    p.segment(y0=\"start\", y1=\"end\", x0=\"name\", x1=\"name\", source=ColumnDataSource(processes), line_width=4, color=\"lime\", alpha=0.6)\n",
    "    p.triangle(\"name\", \"end\", source=ColumnDataSource(processes), size=12, color=\"green\")\n",
    "    p.inverted_triangle(\"name\", \"start\", source=ColumnDataSource(processes), size=8, color=\"lime\")\n",
    "    p.circle(\"src\", \"time\", size=8, source=ColumnDataSource(rpcs), color=\"blue\")\n",
    "    p.segment(y0=\"time\", y1=\"time\", x0=\"src\", x1=\"dst\", source=ColumnDataSource(rpcs), color=\"blue\")\n",
    "    p.circle(\"name\", \"time\", size=8, source=ColumnDataSource(events), color=\"red\")\n",
    "    p.y_range = ranges.Range1d(timerange[1], timerange[0])\n",
    "    p.xaxis.major_label_orientation = math.pi/6\n",
    "    p.sizing_mode = \"scale_width\"\n",
    "    p.height = 400\n",
    "    return p\n",
    "\n",
    "class stat:\n",
    "    def name(): return \"<default>\"\n",
    "    def extract(events): return pd.Series()\n",
    "\n",
    "def filt(events, function): return events[events.apply(function, axis=1)]\n",
    "\n",
    "def timedelta(events, partitioner):\n",
    "    ret = {}\n",
    "    e2 = events.copy()\n",
    "    e2[\"part\"] = e2.apply(partitioner, axis=1)\n",
    "    e2 = e2[e2.apply(lambda x: x is not None, axis = 1)]\n",
    "    for part in e2[\"part\"].unique().tolist():\n",
    "        times = e2[e2[\"part\"] == part][\"time\"].tolist()\n",
    "        if len(times) != 2: continue\n",
    "        ret[part] = (times[1] - times[0]).total_seconds()\n",
    "    return pd.Series(list(ret.values()), list(ret.keys()))\n",
    "\n",
    "class statcol:\n",
    "    def name(): return \"<default>\"\n",
    "    def calc(vals): return None\n",
    "\n",
    "def calcstats(trace, stats, cols):\n",
    "    data = []\n",
    "    for traceid in trace.df[\"traceid\"].unique().tolist():\n",
    "        subtrace = trace.df[trace.df[\"traceid\"] == traceid]\n",
    "        extracted = [ s.extract(subtrace) for s in stats ]\n",
    "        data.extend([ [col.name(), traceid] + [ col.calc(s) for s in extracted ] for col in cols ])\n",
    "    ret = pd.DataFrame(data, columns=[\"stat\", \"traceid\"] + [ s.name() for s in stats ])\n",
    "    ret = ret.set_index([\"traceid\", \"stat\"]).unstack(\"traceid\").transpose().swaplevel().sort_index()\n",
    "    ret.columns.name = None\n",
    "    ret.index = pd.MultiIndex(levels=ret.index.levels, labels=ret.index.labels, names=[None, None])\n",
    "    ret = ret[[ c.name() for c in cols ]]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# FILTERS\n",
    "\n",
    "class remove_cruft(displayfilter):\n",
    "    def fevent(event):\n",
    "        if event.n == \"RPC\":\n",
    "            if event[2].n == \"HeartbeatResponse\": return False\n",
    "            if event[2].n == \"RequestMessage\":\n",
    "                if event[2][2].n == \"Heartbeat\": return False\n",
    "        if event.n == \"BMMUpdate\": return False # Seems to fire on all block put/get/delete requests\n",
    "        if event.n == \"TrackerRegisterShuffle\": return False # Duplicates RegisterShuffle\n",
    "        if event.n == \"Service\": return False # Used to resolve service names\n",
    "        return True\n",
    "    def fservice(host, port, name):\n",
    "        if name == \"\": return False # Unresolved services\n",
    "        if name == \"driverPropsFetcher\": return False\n",
    "        return True\n",
    "\n",
    "class clean_rpcs(displayfilter):\n",
    "    def mutate(tree):\n",
    "        if tree.n == \"RPC\":\n",
    "            tree = tree[2]\n",
    "            if tree.n == \"RequestMessage\": tree = tree[2]\n",
    "        return tree\n",
    "\n",
    "class only_tasks(displayfilter):\n",
    "    pass\n",
    "\n",
    "class remove_events(displayfilter):\n",
    "    def fevent(event):\n",
    "        if event.n == \"RPC\": return True\n",
    "        return False\n",
    "\n",
    "class remove_rpcs(displayfilter):\n",
    "    def fevent(event): return not remove_events.fevent(event)\n",
    "\n",
    "class events_only_block(displayfilter):\n",
    "    def fevent(event):\n",
    "        if event.n == \"RPC\": return True\n",
    "        if event.n in [\"TrackerRegisterShuffle\", \"RegisterShuffle\", \"UnregisterShuffle\", \"BlockFetch\", \"BlockUpload\",\n",
    "            \"GetBlock\", \"GetBlockData\", \"PutBlock\", \"DeleteBlock\", \"FreeBlock\", \"BMMRegister\", \"BMMUpdate\",\n",
    "            \"BMMRemoveBlock\", \"BMMRemoveRDD\", \"BMMRemoveShuffle\", \"BMMRemoveBroadcast\"]: return True\n",
    "        return False\n",
    "\n",
    "class only_management(displayfilter):\n",
    "    def fevent(event):\n",
    "        if event.n == \"RPC\":\n",
    "            if event[2].n.startswith(\"Register\") or event[2].n.startswith(\"Stop\"): return True\n",
    "        if event.n in [\n",
    "            \"DebugMessage\", \"JVMStart\", \"MainStart\", \"MainEnd\", \"SpawnExecutor\", \"StartYarnClient\",\n",
    "            \"SubmittedApplication\", \"SubmitTaskSet\", \"SubmittedTaskSet\", \"ExecutorDone\",\n",
    "            \"DagSchedulerEvent\"\n",
    "        ]: return True\n",
    "        if event.n in [\"SpanStart\", \"SpanEnd\"]:\n",
    "            if event[1].n in [\n",
    "                \"DebugProcess\", \"JVMStart\", \"CreateSparkContext\", \"CreateSparkEnv\", \"YarnAllocate\",\n",
    "                \"FetchDriverProps\"\n",
    "            ]: return True\n",
    "        return False\n",
    "\n",
    "# COLUMNS\n",
    "\n",
    "class stat_count(statcol):\n",
    "    def name(): return \"Count\"\n",
    "    def calc(vals): return len(vals)\n",
    "\n",
    "class stat_min(statcol):\n",
    "    def name(): return \"Min\"\n",
    "    def calc(vals): return vals.min() if len(vals) > 0 else None\n",
    "\n",
    "class stat_max(statcol):\n",
    "    def name(): return \"Max\"\n",
    "    def calc(vals): return vals.max() if len(vals) > 0 else None\n",
    "\n",
    "class stat_mean(statcol):\n",
    "    def name(): return \"Average\"\n",
    "    def calc(vals): return vals.mean() if len(vals) > 0 else None\n",
    "\n",
    "class stat_median(statcol):\n",
    "    def name(): return \"50%\"\n",
    "    def calc(vals): return vals.median() if len(vals) > 0 else None\n",
    "\n",
    "class stat_25p(statcol):\n",
    "    def name(): return \"25%\"\n",
    "    def calc(vals): return vals.quantile(0.25) if len(vals) > 0 else None\n",
    "\n",
    "class stat_75p(statcol):\n",
    "    def name(): return \"75%\"\n",
    "    def calc(vals): return vals.quantile(0.75) if len(vals) > 0 else None\n",
    "\n",
    "class stat_argmin(statcol):\n",
    "    def name(): return \"Min at\"\n",
    "    def calc(vals): return vals.argmin() if len(vals) > 0 else None\n",
    "\n",
    "class stat_argmax(statcol):\n",
    "    def name(): return \"Max at\"\n",
    "    def calc(vals): return vals.argmax() if len(vals) > 0 else None\n",
    "\n",
    "# STATISTICS\n",
    "\n",
    "class data_jvmstart(stat):\n",
    "    def name(): return \"JVM start time\"\n",
    "    def extract(events):\n",
    "        def partition(row):\n",
    "            if row[\"event\"].n not in [\"SpanStart\", \"SpanEnd\"] or row[\"event\"][1].n != \"JVMStart\": return None\n",
    "            return trace.uuid2name(row[\"id\"]) # BAD global variable `trace`\n",
    "        return timedelta(events, partition)\n",
    "\n",
    "class data_execlife(stat):\n",
    "    def name(): return \"Executor lifetime\"\n",
    "    def extract(events):\n",
    "        def partition(row):\n",
    "            if \"sparkExecutor\" not in row[\"name\"]: return None\n",
    "            if not ((row[\"event\"].n == \"MainEnd\") or (row[\"event\"].n == \"SpanEnd\" and row[\"event\"][1].n == \"JVMStart\")): return None\n",
    "            return trace.uuid2name(row[\"id\"]) # BAD global variable `trace`\n",
    "        return timedelta(events, partition)\n",
    "\n",
    "class data_nrpcs(stat):\n",
    "    def name(): return \"RPCs sent\"\n",
    "    def extract(events):\n",
    "        ret = events[events.apply(lambda x: x[\"event\"].n == \"RPC\", axis=1)]\n",
    "        ret[\"sender\"] = ret[\"event\"].apply(lambda x: trace.host2name(x[0])) # BAD global variable `trace`\n",
    "        return ret.groupby(\"sender\").count()[\"id\"]\n",
    "\n",
    "class data_tasklife(stat):\n",
    "    def name(): return \"Task duration\"\n",
    "    def extract(events):\n",
    "        def partition(row):\n",
    "            ev = row[\"event\"]\n",
    "            if ev.n != \"Fn\" or ev[0].n != \"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive\" or len(ev[1]) == 0: return None\n",
    "            if ev[1][0].n not in [\"BeginEvent\", \"CompletionEvent\"]: return None\n",
    "            return (ev[1][0][0][0].n, ev[1][0][0][1].n)\n",
    "        return timedelta(events, partition)\n",
    "\n",
    "class data_instrover(stat):\n",
    "    def name(): return \"Instr overhead\"\n",
    "    def extract(events):\n",
    "        ret = filt(events, lambda x: x[\"event\"].n == \"InstrumentOverhead\") #.apply(lambda x: (trace.uuid2name(x[\"id\"]), int(x[\"event\"][0].n), 0, 0, 0, 0), axis=1) # trace.uuid2name(x[\"id\"])\n",
    "        return pd.Series(ret.apply(lambda x: int(x[\"event\"][0].n), axis=1).tolist(), ret.apply(lambda x: trace.uuid2name(x[\"id\"]), axis=1).tolist())\n",
    "\n",
    "class data_execs(stat):\n",
    "    def name(): return \"Executors started\"\n",
    "    def extract(events):\n",
    "        return filt(events, lambda x: x[\"event\"].n == \"Fn\" and x[\"event\"][0].n == \"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive\" and len(x[\"event\"][1]) > 0 and x[\"event\"][1][0].n == \"ExecutorAdded\")\n",
    "\n",
    "class data_jobs(stat):\n",
    "    def name(): return \"Jobs\"\n",
    "    def extract(events):\n",
    "        return filt(events, lambda x: x[\"event\"].n == \"Fn\" and x[\"event\"][0].n == \"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive\" and len(x[\"event\"][1]) == 0) # WHYYYY? #> 0 and x[\"event\"][1][0].n == \"JobSubmitted\")\n",
    "\n",
    "class data_tasks(stat):\n",
    "    def name(): return \"Tasks\"\n",
    "    def extract(events):\n",
    "        return filt(events, lambda x: x[\"event\"].n == \"Fn\" and x[\"event\"][0].n == \"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive\" and len(x[\"event\"][1]) > 0 and x[\"event\"][1][0].n == \"BeginEvent\")\n",
    "\n",
    "class data_blockupdates(stat):\n",
    "    def name(): return \"Block updates\"\n",
    "    def extract(events):\n",
    "        return filt(events, lambda x: x[\"event\"].n == \"Fn\" and x[\"event\"][0].n == \"org.apache.spark.storage.BlockManagerMaster.updateBlockInfo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base = \"/tmp/spark-trace\"\n",
    "#base = \"/home/matt/code/spark-tracing/runs/remote\"\n",
    "infiles = [base]\n",
    "checkpoint = base + \".pkl\"\n",
    "refresh = True\n",
    "\n",
    "if refresh:\n",
    "    log(\"Reading input\")\n",
    "    trace = reader(infiles)\n",
    "    log(\"Applying preliminary filters\")\n",
    "    trace.filter([remove_cruft])\n",
    "    log(\"Calculating statistics\")\n",
    "    stats = [calcstats(trace,\n",
    "        [data_execs, data_jobs, data_tasks, data_blockupdates],\n",
    "        [stat_count]),\n",
    "    calcstats(trace,\n",
    "        [data_jvmstart, data_nrpcs, data_execlife, data_tasklife, data_instrover],\n",
    "        [stat_count, stat_min, stat_25p, stat_median, stat_75p, stat_max, stat_argmin, stat_argmax])]\n",
    "    log(\"Writing output\")\n",
    "    with open(checkpoint, \"wb\") as outf:\n",
    "        pickle.dump(trace, outf)\n",
    "        pickle.dump(stats, outf)\n",
    "else:\n",
    "    log(\"Reading checkpoint\")\n",
    "    with open(checkpoint, \"rb\") as inf:\n",
    "        trace = pickle.load(inf)\n",
    "        stats = pickle.load(inf)\n",
    "log(\"Applying filters\")\n",
    "trace.filter([])\n",
    "trace.filter([clean_rpcs])\n",
    "log(\"Displaying results\")\n",
    "#trace.debug_display()\n",
    "for s in stats: display(s)\n",
    "show(seqplot(trace))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
