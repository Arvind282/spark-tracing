props = {
  traceout = "file:///tmp/spark-trace"
  result = "/tmp/spark-trace.out"
  overhead = true
  #mode = "dump"
}
targets = {
  "org.apache.spark" = [
    { type = rpc },
    { type = event, class = "scheduler.DAGSchedulerEventProcessLoop",         method = "doOnReceive",           format = "DAGSchedulerEvent($1.0($1.1))" },
    { type = event, class = "scheduler.TaskSchedulerImpl",                    method = "submitTasks",           format = "SubmitTasks($1)" },
    { type = event, class = "scheduler.TaskSchedulerImpl",                    method = "taskSetFinished",       format = "TaskSetFinished" },
    { type = event, class = "storage.BlockManager",                           method = "getBlockData",          format = "GetBlockData($1)" },
    { type = event, class = "storage.BlockManager",                           method = "get",                   format = "BlockManagerGet($1)" },
    { type = event, class = "storage.BlockManager",                           method = "doPut",                 format = "PutBlock($1, $2)" },
    { type = event, class = "storage.BlockManager",                           method = "dropFromMemory",        format = "DropFromMemory($1)" },
    { type = event, class = "storage.BlockManager",                           method = "removeBlock",           format = "ManagerRemoveBlock($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "registerBlockManager",  format = "RegisterBlockManager($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "updateBlockInfo",       format = "UpdateBlockInfo($1, $2, $3)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "removeBlock",           format = "MasterRemoveBlock($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "removeRdd",             format = "RemoveRdd($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "removeShuffle",         format = "RemoveShuffle($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "removeBroadcast",       format = "RemoveBroadcast($1)" },
    { type = event, class = "network.netty.NettyBlockTransferService",        method = "fetchBlocks",           format = "FetchBlocks($1:$2 $3, $4)" },
    { type = event, class = "network.netty.NettyBlockTransferService",        method = "uploadBlock",           format = "UploadBlock($1:$2 $3, $4)" },
    { type = event, class = "shuffle.sort.SortShuffleManager",                method = "registerShuffle",       format = "SortShuffleManager.RegisterShuffle($1, $2)" },
    { type = event, class = "shuffle.sort.SortShuffleManager",                method = "unregisterShuffle",     format = "SortShuffleManager.UnregisterShuffle($1)" },
    { type = event, class = "MapOutputTrackerMaster",                         method = "registerShuffle",       format = "MapOutputTrackerMaster.RegisterShuffle($1, $2)" }, # TODO Are these two redundant with the prior two?
    { type = event, class = "MapOutputTrackerMaster",                         method = "unregisterShuffle",     format = "MapOutputTrackerMaster.UnregisterShuffle($1)" },
  # { type = event, class = "SparkFirehoseListener",                          method = "onEvent",               format = "ListenerEvent($1)" }, # Very verbose, and I'm not sure how useful it really is
    { type = span,  class = "rpc.RpcEnv$",                                    method = "create",                format = "CreateRpcEnv($1)" }, # Instrumenting NettyRpcEnvFactory.create doesn't give results, for some reason
    { type = span,  class = "scheduler.cluster.YarnClientSchedulerBackend",   method = "waitForApplication",    format = "WaitOnBackend" },
    { type = span,  class = "SparkContext",                                   method = "createSparkEnv",        format = "CreateSparkEnv" },
    { type = span,  class = "SparkContext",                                   method = "SparkContext",          format = "CreateSparkContext" },
    { type = span,  class = "metrics.MetricsSystem",                          method = "start",                 format = "StartMetricsSystem" },
    { type = span,  class = "scheduler.TaskSchedulerImpl",                    method = "waitBackendReady",      format = "TaskSchedulerWaitBackend" },
  # { type = span,  class = "deploy.yarn.YarnAllocator",                      method = "allocateResources",     format = "AllocateResources" },
    { type = span,  class = "deploy.yarn.ExecutorRunnable",                   method = "startContainer",        format = "StartContainer" },
    { type = span,  class = "deploy.yarn.ApplicationMaster",                  method = "org$apache$spark$deploy$yarn$ApplicationMaster$$waitForSparkDriver",   format = "" },
    { type = span,  class = "deploy.yarn.ApplicationMaster",                  method = "org$apache$spark$deploy$yarn$ApplicationMaster$$registerAM",           format = "" },
    { type = span,  class = "deploy.yarn.Client",                             method = "org$apache$spark$deploy$yarn$Client$$distribute$1",                    format = "" },
    { type = span,  class = "deploy.yarn.Client",                             method = "prepareLocalResources", format = "PrepareLocalResources($1)" },
    { type = span,  class = "deploy.yarn.Client",                             method = "submitApplication",     format = "SubmitApplication($r)" },
  # { type = span,  class = "deploy.yarn.Client",                             method = "Client",                format = "" }, # FIXME Doesn't construct properly for some reason, so we get NPEs.
  # Example local variable capture:
  # { type = local, class = "storage.BlockManager",                           method = "doPut",                 variable = "startTimeMs",                      format = "StartPut($2 = $3)" },
  # TODO Need FetchDriverProps in CoarseGrainedExecutorBackend.run
  ]
  "org.apache.toree" = [
    { type = span,  class = "boot.KernelBootstrap",                                      method = "initialize",             format = "KernelBootstrapInitialize" },
    { type = span,  class = "boot.layer.StandardComponentInitialization$class",          method = "initializeComponents",   format = "InitializeComponents" },
    { type = span,  class = "plugins.PluginManager",                                     method = "initialize",             format = "InitializePlugins" },
    { type = span,  class = "kernel.interpreter.scala.ScalaInterpreterSpecific$class",   method = "start",                  format = "StartScalaInterpreter" },
    { type = span,  class = "kernel.interpreter.scala.ScalaInterpreter",                 method = "bindVariables",          format = "BindInterpreterVariables" },
  # { type = span,  class = "kernel.interpreter.scala.ScalaInterpreter",                 method = "init",                   format = "StartScalaInterpreter" }, # FIXME javassist.CannotCompileException: cannot find org.apache.toree.interpreter.Interpreter
  # { type = span,  class = "boot.layer.InterpreterManager",                             method = "initializeInterpreters", format = "InitializeInterpreters" },
  ]
}
filters = {
  "3.0 = RPC" {
    "3.3.0 = Heartbeat" = false
    "3.3.0 = HeartbeatResponse" = false
  }
}
remove-services = [".*driverPropsFetcher"]
case-parse = {
  # TODO How about accumulators?
  "3.0 = Fn" = {
    "default" = ["3.1"]
    "3.2.1.0 in BeginEvent, CompletionEvent" = ["3.2.1.1"]
    "3.1.0 = org.apache.spark.storage.BlockManagerMaster.registerBlockManager" = ["3.2.1", "3.2.4"]
    "3.1.0 = org.apache.spark.storage.BlockManager.doPut" = ["3.2.2"]
    "3.1.0 = org.apache.spark.storage.BlockManagerMaster.updateBlockInfo" = ["3.2.1", "3.2.3"]
  }
  "3.0 = LocalVariable" = ["3.1"]
  "3.2.0 = Fn" = ["3.2.1"]
  "3.0 = RPC" {
    "3.3.0 = RegisterBlockManager" = ["3.3.4"]
    "3.3.0 = RegisterExecutor" = ["3.3.2"]
    "3.3.0 = UpdateBlockInfo" = ["3.3.3"]
    "3.3.0 = RegisterClusterManager" = ["3.3.1"]
    "3.3 ~ ^BlockManagerId\\(" = ["3.3"]
    "3.3.1 ~ ^BlockManagerId\\(" = ["3.3.1"]
  }
}
