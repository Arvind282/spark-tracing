props = {
  traceout = "file:///tmp/spark-trace"
  result = "/tmp/spark-trace.out"
  overhead = true
}
targets = {
  "org.apache.spark" = [
    { type = rpc },
    { type = event, class = "scheduler.DAGSchedulerEventProcessLoop",         method = "doOnReceive",           format = "DAGSchedulerEvent($1.0($1.1))" },
    { type = event, class = "scheduler.TaskSchedulerImpl",                    method = "submitTasks",           format = "SubmitTasks($1)" },
    { type = event, class = "scheduler.TaskSchedulerImpl",                    method = "taskSetFinished",       format = "TaskSetFinished" },
    { type = event, class = "storage.BlockManager",                           method = "getBlockData",          format = "GetBlockData($1)" },
    { type = event, class = "storage.BlockManager",                           method = "get",                   format = "BlockManagerGet($1)" },
    { type = event, class = "storage.BlockManager",                           method = "doPut",                 format = "PutBlock($1, $2)" },
    { type = event, class = "storage.BlockManager",                           method = "dropFromMemory",        format = "DropFromMemory($1)" },
    { type = event, class = "storage.BlockManager",                           method = "removeBlock",           format = "ManagerRemoveBlock($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "registerBlockManager",  format = "RegisterBlockManager($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "updateBlockInfo",       format = "UpdateBlockInfo($1, $2, $3)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "removeBlock",           format = "MasterRemoveBlock($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "removeRdd",             format = "RemoveRdd($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "removeShuffle",         format = "RemoveShuffle($1)" },
    { type = event, class = "storage.BlockManagerMaster",                     method = "removeBroadcast",       format = "RemoveBroadcast($1)" },
    { type = event, class = "network.netty.NettyBlockTransferService",        method = "fetchBlocks",           format = "FetchBlocks($1:$2 $3, $4)" },
    { type = event, class = "network.netty.NettyBlockTransferService",        method = "uploadBlock",           format = "UploadBlock($1:$2 $3, $4)" },
    { type = event, class = "shuffle.sort.SortShuffleManager",                method = "registerShuffle",       format = "SortShuffleManager.RegisterShuffle($1, $2)" },
    { type = event, class = "shuffle.sort.SortShuffleManager",                method = "unregisterShuffle",     format = "SortShuffleManager.UnregisterShuffle($1)" },
    { type = event, class = "MapOutputTrackerMaster",                         method = "registerShuffle",       format = "MapOutputTrackerMaster.RegisterShuffle($1, $2)" }, # TODO Are these two redundant with the prior two?
    { type = event, class = "MapOutputTrackerMaster",                         method = "unregisterShuffle",     format = "MapOutputTrackerMaster.UnregisterShuffle($1)" },
  # { type = event, class = "SparkFirehoseListener",                          method = "onEvent",               format = "ListenerEvent($1)" }, # Very verbose, and I'm not sure how useful it really is
    { type = span,  class = "rpc.RpcEnv$",                                    method = "create",                format = "CreateRpcEnv($1)" }, # Instrumenting NettyRpcEnvFactory.create doesn't give results, for some reason
    { type = span,  class = "scheduler.cluster.YarnClientSchedulerBackend",   method = "waitForApplication",    format = "YarnClientSchedulerWaitBackend" },
    { type = span,  class = "SparkContext",                                   method = "createSparkEnv",        format = "CreateSparkEnv" },
    { type = span,  class = "SparkContext",                                   method = "SparkContext",          format = "CreateSparkContext" },
    { type = span,  class = "metrics.MetricsSystem",                          method = "start",                 format = "StartMetricsSystem" },
    { type = span,  class = "scheduler.TaskSchedulerImpl",                    method = "waitBackendReady",      format = "TaskSchedulerWaitBackend" },
    { type = span,  class = "deploy.yarn.YarnAllocator",                      method = "allocateResources",     format = "YarnAllocatorAllocateResources" },
    { type = span,  class = "deploy.yarn.ExecutorRunnable",                   method = "startContainer",        format = "YarnApplicationManagerStartContainer" },
    { type = span,  class = "deploy.yarn.ApplicationMaster",                  method = "org$apache$spark$deploy$yarn$ApplicationMaster$$waitForSparkDriver",   format = "" },
    { type = span,  class = "deploy.yarn.ApplicationMaster",                  method = "org$apache$spark$deploy$yarn$ApplicationMaster$$registerAM",           format = "" },
    { type = span,  class = "deploy.yarn.Client",                             method = "org$apache$spark$deploy$yarn$Client$$distribute$1",                    format = "" },
  # { type = span,  class = "deploy.yarn.Client",                             method = "prepareLocalResources", format = "" },
  # { type = span,  class = "deploy.yarn.Client",                             method = "submitApplication",     format = "" },
  # { type = span,  class = "deploy.yarn.Client",                             method = "Client",                format = "" }, # FIXME Doesn't construct properly for some reason, so we get NPEs.
  # TODO Need FetchDriverProps in CoarseGrainedExecutorBackend.run
  ]
}
filters = {
  "3.0 = RPC" {
    "3.3.0 = Heartbeat" = false
    "3.3.0 = HeartbeatResponse" = false
  }
}
remove-services = [".*driverPropsFetcher"]
